# BTAP Batch
BTAP Batch allows you to run paramteric and optimization analysis on your local machine and on the Amazon Cloud. You can
select the parameters you wish to analyse by modifying an input text file, run the simulation and it will produce the 
simulation outputs files for each simulation. It will also provide a high level data summary excel file.  

## Why BTAP Batch?
BTAP Batch simplifies the process to run parametric and optimization runs. It uses code directly from 
openstudio-standards and requires no PAT measures. 

During development using branches of btap_costing and openstudio_standards using OpenStudio Server has been problematic. 
BTAP Batch solves this problem and allows us to use any combination of branches in large runs. 

Using AWS batch also reduces the cost of simulations and enables researchers to use AWS dashboards to monitor the simulation runs.  
 
## Requirements
* Windows 10 Professional version 1909 or greater
* [Docker](https://docs.docker.com/docker-for-windows/install/) running on your computer.
* A python **miniconda** environment [3.8](https://docs.conda.io/en/latest/miniconda.html)
* A git [client](https://git-scm.com/downloads)
* A high speed internet connection.
* A github account and [git-token](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token)
* Permissions to access canmet-energy repositories from phylroy.lopez@canada.ca
* [NRCan btap_dev AWS account credentials set up on your computer](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html) for Amazon HPC runs. 
* Jupyter Lab (Optional)
* Pycharms (Optional)

## Configuration

0. Initialize your windows system to work with conda environments using windows powershell. Type this in a windows 
powershell console.
```
conda init powershell
```
1. Clone this repository to your computer and change into the project folder using windows powershell.
```
git clone <add repository>
cd btap_batch
```
2. Set up your conda/python environment 'btap_batch_env. This will download all required packages to your system.  
For those familiar with Ruby, this is similar to a Gemfile/Bundle environment. This includes Jupyter Lab. 
```
conda env create -f environment.yml
```

3. Activate your conda environment. 
```
conda activate btap_batch_env
```

4. Make this environment available to Jupyter (Optional)
```
python -m ipykernel install --user --name=btap_batch_env
```

## QuickStart Command Line 

### Parametric Analysis Local
1. To run a parametric analysis, go to the example.yml analysis file in the 'example' folder. Each 
parameter is explained in that file. Ensure that parametric analysis is a reasonable size for your system. Ensure that 
the ':compute_environment' variable is set to local. 

2. Open the example.py file in the same folder. Edit the file to add you git_api_token. The comments in the file will
indicate which switches do what. Do not commit your git token. 

3. Run the example.py file from the root of the btap_batch project folder. On Windows you will need to set the 
PYTHONPATH to the to that folder. Please ensure that the btap_batch_env environment is active. 
```
set PYTHONPATH=%cd% && python example\example.py
```
4. Simulation should start to run. A folder will be created in parametric folder with the variable name you set 
':analysis_name' in the yml file. It will create a unique folder under this based on a random UUID for this analysis. In 
that folder you will see two folders, 'input' and 'output'. 

* The input folder contains folders with uuids for all the simulations that you are running. It contains the 
run_options.yml file. This contains the selections created for that particular run. 

* The output folder contains full output runs for all the local simulation. This also contains the output.xlsx file 
with high level information from all the simulations. 

## Parametric AWS
1. Update your AWS credentials. 
1. Change ':compute_environment' to aws_batch.
1. Follow the same Local instruction above 1-4. 

The output of the runs is not stored locally, but on the S3 Bucket,  the ':analysis_name' you chose and the analysis_id 
generated by the script. For example, if you ran the analysis with the analysis_name: set to 'example_analysis' and the default 
:s3_bucket is your aws username is 'phylroy.lopez'. The runs would be kept on S3 in a path like
s3://<:s3_bucket_name>/<your_user_name>/<:analysis_name>/<:analysis_id>/

*Note*: This script will not delete anything from S3. So you must delete your S3 folders yourself.

The excel output will be save on your local machine in the output folder for the run. 

## Optimization
To perform an optimization run. 
1. Replace the ':algorithm parametric' of information in the example.yml file with this block.
```
  :algorithm:
    :type: nsga2 
    :population: 100
    :n_generations: 5
    :prob: 0.85
    :eta: 3.0
    :minimize_objectives: [
#       "cost_utility_ghg_total_kg_per_m_sq",
#       "energy_eui_total_gj_per_m_sq",
        "cost_utility_neb_total_cost_per_m_sq",
        "cost_equipment_total_cost_per_m_sq"
    ]
```
The algorithm type is always nsga2. The number of population and generations determin how big the simulation will be. 
For example,  the above optimization will run 100 individuals for 5 generations, for a possible maximum of 500. 

The minimized_objectives can be anything in the btap_data.json file. You can view the output of a btap_data.json file 
from a local parametric run. However most of the time the above variables would be sufficient to optimize building designs.

For more details on the nsga algorithm please visit the pymoo website. 

To run the optimization, follow the steps 1-4 as above. 

## Monitoring the Analysis
While the program will output items to the console, there are a few other ways to monitor the results if you wish 

### PostGreSQL 
While the simulations are running, you have access to the postgres database. Note that this database will abruptly shutdown when 
the simulations are complete by design. You can use database viewers like DBeaver or pgAdmin. The username and password for the 
database is 'docker'. You can also optionally build a viewer via a Jupyter Note using python's SQLAlchemy library with postgresql. 

### Amazon Web Services
If you are running on aws-batch. You can monitor the simulation in the AWS Batch Dashboard and the compute resources 
being used in the EC2 dashboard. 

# PowerBI / Tableau
Through the postgresSQL server you can connect and update live data using either of these tools. 